import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import os
import re
import requests
from urllib.parse import urlparse


def is_valid_media_url(url):
    """ç”¨ä¾†éæ¿¾éè²¼æ–‡å…§å®¹çš„åœ–ï¼ˆåƒé ­åƒï¼‰"""
    return re.search(r"instagram.*(fbcdn\.net|cdninstagram\.com)", url) and (
        re.search(r"\.jpg|\.jpeg|\.png|\.mp4", url)
    )

def get_post_author(html):
    """å–å¾—è²¼æ–‡ä½œè€…çš„åç¨±èˆ‡å¸³è™Ÿ"""
    soup = BeautifulSoup(html, "html.parser")
    meta = soup.find("meta", {"name": "twitter:title"})
    print("get_post_author:", meta)
    if not meta:
        return None, None

    content = meta.get("content", "")
    match = re.search(r"Threads ä¸Šçš„(.+?)ï¼ˆ(@[^ï¼‰]+)ï¼‰", content)
    if match:
        return match.group(1), match.group(2)
    match = re.search(r"(.+?)\s+\((@[^)]+)\)\s+on Threads", content)
    if match:
        return match.group(1), match.group(2)
    return None, None

async def extract_media_urls(page):
    """æ“·å–è²¼æ–‡å…§æ‰€æœ‰çš„åœ–ç‰‡èˆ‡å½±ç‰‡ URL"""
    # æŠ“å–æ–‡ç« å…§çš„åœ–ç‰‡èˆ‡å½±ç‰‡å…ƒç´ 
    # post_div = page.locator("div[aria-label='ç›´æ¬„å…§æ–‡'] > div").first
    post_divs = page.locator("div[aria-label='ç›´æ¬„å…§æ–‡'] > div")
    post_div = post_divs.nth(0)
    with open("post_div.html", "w", encoding="utf-8") as f:
        f.write(await post_div.inner_html())
    img_elements = await post_div.locator("img[src]:not([alt*='å¤§é ­è²¼ç…§'])").all()
    video_elements = await post_div.locator("video[src]").all()

    image_urls = set()
    video_urls = set()

    # å„²å­˜åœ–ç‰‡ HTML
    with open("elements.html", "w", encoding="utf-8") as f:
        for i, img in enumerate(img_elements):
            src = await img.get_attribute("src")
            if src and is_valid_media_url(src):
                image_urls.add(src)
                outer_html = await img.evaluate("e => e.outerHTML")
                f.write(f"{i+1}: {src}\n{outer_html}\n\n")
        for i, video in enumerate(video_elements):
            src = await video.get_attribute("src")
            if src and is_valid_media_url(src):
                video_urls.add(src)
                outer_html = await video.evaluate("e => e.outerHTML")
                f.write(f"{i+1}: {src}\n{outer_html}\n\n")
    return list(image_urls), list(video_urls)

async def scrape_threads_post(post_url, download_folder="downloads"):
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=False,
            executable_path="C:/Program Files/Google/Chrome/Application/chrome.exe"
            )
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
            bypass_csp=True,
        )
        page = await context.new_page()
        await page.goto(post_url, wait_until="domcontentloaded")

        await page.wait_for_selector("body", timeout=10000)
        
        html = await page.content()

        # æ“·å–ä½œè€…è³‡è¨Š
        realname, username = get_post_author(html)
        if not realname or not username:
            print("âŒ ç„¡æ³•è§£æä½œè€…è³‡æ–™")
            return
        
        print(f"ğŸ‘¤ ä½œè€…ï¼š{realname}ï¼ˆ{username}ï¼‰")

        # è¨­å®šå„²å­˜è³‡æ–™å¤¾
        save_dir = os.path.join(download_folder, f"{realname}_{username}")
        os.makedirs(save_dir, exist_ok=True)

        # input("æŒ‰ Enter éµç¹¼çºŒ...")

        image_urls, video_urls = await extract_media_urls(page)
        print(f"ğŸ“¸ åœ–ç‰‡æ•¸ï¼š{len(image_urls)}")
        print(f"ğŸ¥ å½±ç‰‡æ•¸ï¼š{len(video_urls)}")

        # ä¸‹è¼‰åœ–ç‰‡
        for i, url in enumerate(image_urls):
            ext = os.path.splitext(urlparse(url).path)[-1].split("?")[0] or ".jpg"
            filename = f"image_{i+1}{ext}"
            filepath = os.path.join(save_dir, filename)
            try:
                with open(filepath, "wb") as f:
                    f.write(requests.get(url).content)
                print(f"âœ… å·²ä¸‹è¼‰åœ–ç‰‡ï¼š{filename}")
            except Exception as e:
                print(f"âŒ åœ–ç‰‡ä¸‹è¼‰å¤±æ•— {url}: {e}")

        # ä¸‹è¼‰å½±ç‰‡
        for i, url in enumerate(video_urls):
            ext = os.path.splitext(urlparse(url).path)[-1].split("?")[0] or ".mp4"
            filename = f"video_{i+1}{ext}"
            filepath = os.path.join(save_dir, filename)
            try:
                with open(filepath, "wb") as f:
                    f.write(requests.get(url).content)
                print(f"âœ… å·²ä¸‹è¼‰å½±ç‰‡ï¼š{filename}")
            except Exception as e:
                print(f"âŒ å½±ç‰‡ä¸‹è¼‰å¤±æ•— {url}: {e}")

        await browser.close()


if __name__ == "__main__":
    url = "è¦çˆ¬å–çš„threadsè²¼æ–‡ç¶²å€"
    asyncio.run(scrape_threads_post(url))
